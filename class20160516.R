---
  title: "大數據分析方法"
author: "曾意儒 Yi-Ju Tseng, 長庚大學資管系"
date: "May 16, 2016"
output: ioslides_presentation
subtitle: "Machine Learning and Data Mining"
highlighter: highlight.js
---
  
  # ggplot2 複習
  
  ## 基本架構
  - Data `ggplot(Data)`
- Geometries `geom_line()`, `geom_bar()`, `geom_point()`
- Aesthetic properties `ggplot(Data,aes(x=1, y=1, color='black'))`
- Scale mappings `scale_fill_continuous()`, `scale_fill_grey()`
- Stats `stat_density2d()`

## Geoms: 1 Variable
<img src="Fig/gg1.png" height="500px">
  
  ## Geoms: 2 Variables
  <img src="Fig/gg3.png" width="800px">
  
  ## Geoms: 2 Variables
  <img src="Fig/gg4.png" width="800px">
  
  ## Geoms: Graphical Primitives
  <img src="Fig/gg2.png" height="500px">
  
  ## 參考資料
  [ggplot2 Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)

# Clustering

## Clustering
- Hierarchical Clustering 階層式分群法, `dist()`, `hclust()`
- K-means Clustering, `kmeans()`

<img src="Fig/hc.png" width="300px"><img src="Fig/kmeans.gif" width="300px">
  
  
  ## Hierarchical Clustering步驟
  - 定義`距離`還有`聚合方式`
- 計算倆倆觀察值的距離
- 找到距離最小的兩點，合成一組
- 找到距離次小的兩點，合成一組


<img src="Fig/hcstep.jpg" width="600px">
  
  ## K-means Clustering步驟
  - 預設中心
- 用預設中心分群
- 用分群重算中心
- 再用中心分群....

<img src="Fig/kmeansstep.png" width="700px">
  
  
  ## K-means注意事項
  
  - 需要決定# of clusters
- 用眼睛/人工/特殊要求選
- 用 cross validation/information theory選
- [Determining the number of clusters](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)


- K-means 沒有一定的結果
- 不同的 # of clusters 
- 不同的 # of iterations


## `kmeans()`, k=2

```{r,echo=F,fig.height=5,fig.width=4}
x<-scale(mtcars$hp[-1]);y<-scale(mtcars$mpg[-1])
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=2)
par(mar=rep(0.2,4),mfrow=c(1,1))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:2,pch=3,cex=3,lwd=3)
```

## `kmeans()`, k=3

```{r,echo=F,fig.height=5,fig.width=4}
x<-scale(mtcars$hp[-1]);y<-scale(mtcars$mpg[-1])
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=3)
par(mar=rep(0.2,4),mfrow=c(1,1))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
```

## `kmeans()`, k=4

```{r,echo=F,fig.height=5,fig.width=4}
x<-scale(mtcars$hp[-1]);y<-scale(mtcars$mpg[-1])
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=4)
par(mar=rep(0.2,4),mfrow=c(1,1))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:4,pch=3,cex=3,lwd=3)
```



## 上課用程式碼
[Rmd下載](https://github.com/yijutseng/BigDataCGUIM/blob/master/BigData20160516.Rmd)

按Raw，右鍵另存新檔

投影片下載：

[Html下載](https://raw.githubusercontent.com/yijutseng/BigDataCGUIM/master/BigData20160516.html)

按右鍵，另存新檔



## Use sum of squared error (SSE) scree plot to optimize the number of clusters

SSE: The sum of the squared distance between each member of a cluster and its cluster centroid.

[參考資料](http://stackoverflow.com/questions/15376075/cluster-analysis-in-r-determine-the-optimal-number-of-clusters)

<img src="Fig/SSE.png" width="500px">
  
  ## SSE screen plot `withinss`
  ```{r,fig.height=3,fig.width=8}
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
wss <- (nrow(dataMatrix)-1)*sum(apply(dataMatrix,2,var))
for (i in 2:(nrow(dataMatrix)-1)) {
  wss[i] <- sum(kmeans(dataMatrix,centers=i)$withinss)
}
par(mfrow=c(1,1), mar = c(4,4,1,1)) #下,左,上,右
plot(1:(nrow(dataMatrix)-1), wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
```


## Missing values ###用來看null

```{r,error=TRUE}
dataMatrix2 <- as.matrix(mtcars)
## Randomly insert some missing data
dataMatrix2[sample(1:100,size=10,replace=FALSE)] <- NA
head(dataMatrix2,10)
```




## Imputing {impute}
用knn的方法計算空值可能可以帶入的數值(補值)
```{r,fig.height=4,fig.width=8,tidy=FALSE}
#source("https://bioconductor.org/biocLite.R")
#biocLite("impute")
library(impute)
dataMatrix2 <- impute.knn(dataMatrix2)$data
head(dataMatrix2,10)
```



# Machine Learning 機器學習

## 什麼是Machine Learning？
- Learning from data
- `Machine` learning from data
<img src="Fig/ML.png" width="700px">
  
  ## 什麼是Machine Learning？
  - 建立可以從`輸入資料`學習`新資訊`，變成`智慧`的演算法
- 演算法可以基於`輸入資料`，`預測事件`或`協助決策`

- 資料太`少`？`太髒`？-->學不好

Data ---- `Machine Learning` ----> Skill 

Skill: 變準/變好/賺更多...etc

## Machine Learning vs. Artificial Intelligence
- ML: use data to compute something that improves performance
- AI: compute something that shows `intelligent` behavior

`ML` is one possible and popular route to realize `AI`

## Machine Learning無所不在
- 天氣預測
- 搜尋建議、購物建議
- 股市預測
- 臉部辨識、指紋辨識
- 垃圾郵件標記
- 尿布啤酒

## 什麼時候需要ML?
- 有一些模式/模型可`學`
- 很難定義這些模式/模型
- 有資料可`學`這些模式/模型

## Machine Learning步驟
<img src="Fig/MLstep.jpg" width="700px">
  
  ## Learning 種類
  - Classification 分類
- 分兩類（P/N, Yes/No, M/F, Sick/Not sick）
- 分多類 (A/B/C/D)

- Regression 迴歸
- 真實的'值'（股票、氣溫）

- Ranking
- 排序（Google page rank）

- Clustering
- 分群

## Machine Learning 種類 -1
- Supervised learning 監督式學習
- Regression 迴歸
- Linear Regression 線性迴歸
- Logistic Regression 羅吉斯迴歸、邏輯迴歸
- Classification 分類
- Support Vector Machines 支持向量機
- Decision Trees 決策樹
- Neural Networks 神經網路
- K-Nearest Neighbor 

## Supervised learning 監督式學習
<img src="Fig/SupervisedLearning.png" width="700px">
  
  [出處](http://www.astroml.org/sklearn_tutorial/general_concepts.html)

## Machine Learning 種類 -2
- Unsupervised learning 非監督式學習
- Clustering! 分群
- Hierarchical clustering 階層式分群
- K-means clustering
- Association Rules 關聯式規則

## Unsupervised learning 非監督式學習
<img src="Fig/UnsupervisedLearning.png" width="700px">
  
  [出處](http://www.astroml.org/sklearn_tutorial/general_concepts.html)

## 怎麼選Algorithms?
<img src="Fig/MLCS.png" width="800px">
  
  ## Regression Analysis 迴歸分析
  了解兩個或多個變數間`是否相關`、`相關方向與強度`，並建立`數學模型`以便觀察特定變數來預測研究者感興趣的變數

- Linear Regression 線性迴歸
- Logistic Regression 羅吉斯迴歸、邏輯迴歸

[Wiki](https://zh.wikipedia.org/wiki/%E8%BF%B4%E6%AD%B8%E5%88%86%E6%9E%90)

## Linear Regression 線性迴歸
<img src="Fig/LR.jpg" width="700px">
  
  [出處](http://www2.cedarcrest.edu/academic/bio/hale/biostat/session24links/regression.html)

## Logistic Regression 邏輯迴歸
<img src="Fig/LogR.png" width="800px">
  
  ## Regression Analysis 迴歸分析
  - `Linear Regression 線性迴歸`
- Logistic Regression 羅吉斯迴歸、邏輯迴歸

## 來用在NBA的資料看看！
```{r message=FALSE,warning=FALSE}
#讀入SportsAnalytics package
if (!require('SportsAnalytics')){
  install.packages("SportsAnalytics")
  library(SportsAnalytics)
}
#擷取2015-2016年球季球員資料
NBA1516<-fetch_NBAPlayerStatistics("15-16")
```

## NBA`得分`與`上場分鐘數`的線性迴歸分析
```{r warning=F,message=F,fig.height=4}
library(ggplot2)
ggplot(NBA1516,aes(x=TotalMinutesPlayed,y=TotalPoints))+
  geom_point()+geom_smooth(method = "glm")
```

## 簡單線性迴歸分析- `glm()`
```{r warning=F,message=F,fig.height=4.5}
# formula: Y~X1+X2+...+Xn  Y:依變項 X:自變項
# data: 資料
glm(TotalPoints~TotalMinutesPlayed,data =NBA1516)  ##前面是x後面是y
```

TotalPoints = `0.4931` * TotalMinutesPlayed `-85.9071`
##0.4931*打得分鐘數 - 85.9071

## 其實`glm()`是廣義線性迴歸模型
- generalized linear models (glm)
- 包括了線性迴歸模型和邏輯迴歸模型
- 線性模型也可用`lm()`
- 如何修改預設模型？
- `family="gaussian"` 線性模型模型
- `family="binomial"` 邏輯迴歸模型
- `family="poisson"` 卜瓦松迴歸模型

## Gaussian distribution
高斯函數是`常態分布`的密度函數

<img src="Fig/gaussian.gif" width="700px">
  
  ## Binomial distribution
  二項分布是`n個獨立的是/非試驗中成功的次數`的離散機率分布

<img src="Fig/binomial.gif" width="650px">
  
  ## Poisson distribution
  `次數`分佈

- 某一服務設施在一定時間內受到的服務請求的次數
- 公車站的候客人數
- 機器故障數
- 自然災害發生的次數
- DNA序列的變異數.....

<img src="Fig/poisson.png" width="300px">
  
  ## `得分`與`上場分鐘數`和`兩分球出手數`的關係 - 多變量線性迴歸分析
  
  ```{r warning=F,message=F,fig.height=4.5}
# e+01: 10^1 / e-04: 10^(-4)
glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted,
    data =NBA1516)
```

TotalPoints = `-0.0002347` * TotalMinutesPlayed + `1.255794` *FieldGoalsAttempted  `-17.99`


## `得分`與`上場分鐘數`和`兩分球出手數`和`守備位置`的關係 - 多變量線性迴歸分析

```{r warning=F,message=F,fig.height=4.5}
glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted+Position,
    data =NBA1516)
# e+01: 10^1 / e-04: 10^(-4)
```

TotalPoints = `-0.0065` * TotalMinutesPlayed + `1.28` *FieldGoalsAttempted  `+22.85` + `22.85` * PositionPF + `-65.03` * PositionPG + `-38.52` * PositionSF + `-52.18` * PositionSG

## 虛擬變項 Dummy Variable 
- PositionPF? PositionPG? PositionSF? PositionSG?
- 如果是控球後衛（PG），會得到...
- PositionPF=0
- PositionPG=1
- PositionSF=0
- PositionSG=0
- 中鋒去哪了？---基準項
- PositionPF=0
- PositionPG=0
- PositionSF=0
- PositionSG=0

## 多變量線性迴歸分析
- 假設：各變數相互獨立！
- 若自變項X是類別變項，需要建立`虛擬變項`
- 在R裡，`類別變項`請記得轉成factor，R會自動建立`虛擬變項`
- 用在`依變數為連續變數`，`自變數為連續變數或虛擬變數`的場合

```{r warning=F,message=F,fig.height=4.5}
class(NBA1516$Position)
levels(NBA1516$Position)
```


## 用哪個計算比較準？
- 可能不一定有適合的模型
- 常用的判斷準則
- Akaike’s Information Criterion (AIC)
- Bayesian Information Criterion (BIC)
- 數值越小越好


## 用哪個計算比較準？
```{r warning=F,message=F,fig.height=4.5}
OneVar<-glm(TotalPoints~TotalMinutesPlayed,data =NBA1516)
TwoVar<-glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted,
            data =NBA1516)
ThreeVar<-glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted+Position,
              data =NBA1516)
c(OneVar$aic,TwoVar$aic,ThreeVar$aic)
```

## 所有參數都有用嗎？
```{r warning=F,message=F,fig.height=4.5}
sum2<-summary(TwoVar)
sum2$coefficients
```

## 所有參數都有用嗎？
```{r warning=F,message=F,fig.height=4.5}
sum3<-summary(ThreeVar)
sum3$coefficients
```

## Regression Analysis 迴歸分析
- Linear Regression 線性迴歸
- `Logistic Regression 羅吉斯迴歸`

## Logistic Regression 羅吉斯迴歸
- 用在`依變數為二元變數（非0即1）`的場合
- 生病/沒生病
- 錄取/不錄取
- `family="binomial"` 邏輯迴歸模型

## 為什麼錄取/不錄取？
```{r warning=F,message=F,fig.height=4.5}
mydata <- read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
# GRE:某考試成績, GPA:在校平均成績, rank:學校聲望
head(mydata)
```

## Hmm....
```{r warning=F,message=F,fig.height=4.5}
mydata$rank <- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank, 
               data = mydata, family = "binomial")
sum<-summary(mylogit)
sum$coefficients
```

# Decision Trees 決策樹

## 什麼是決策樹？
在`樹狀目錄`中建立一系列分割，以建立模型。這些分割會表示成`「節點」(Node)`。每次發現輸入資料行與可預測資料行有明顯地相互關聯時，此演算法就會在模型中加入一個`節點`。演算法決定分岔的方式不同，視它預測連續資料行或分隔資料行而定。

<img src="Fig/DT.png" width="500px">
  
  ## 決策樹的種類
  - Classificaiton
- Regression
- Classification And Regression Tree (CART)


## 用籃板/三分/助攻/抄截來判斷位置
```{r warning=F,message=F,fig.height=4.5}
if (!require('rpart')){
  install.packages("rpart")
  library(rpart)
}
DT<-rpart(Position~Blocks+ThreesMade+Assists+Steals,data=NBA1516)
DT
#控球後衛（PG）、得分後衛（SG）、小前鋒（SF）、大前鋒（PF）和中鋒（C）
```

## 用籃板/三分/助攻/抄截來判斷位置
```{r warning=F,message=F,fig.height=4.5}
par(mfrow=c(1,1), mar = rep(1,4)) #下,左,上,右
plot(DT)
text(DT, use.n=F, all=F, cex=1)
#控球後衛（PG）、得分後衛（SG）、小前鋒（SF）、大前鋒（PF）和中鋒（C）
```

## 用籃板/三分/助攻/抄截來判斷位置
預設的`plot()`真的太難用，改用`rpart.plot` package裡面的`prp()`
```{r warning=F,message=F,fig.height=4.5}
if (!require('rpart.plot')){
  install.packages("rpart.plot")
  library(rpart.plot)
}
prp(DT)					# Will plot the tree
```

## 用籃板/三分/助攻/抄截來判斷位置
```{r warning=F,message=F,fig.height=5}
prp(DT)
```

## 決策樹演算法怎麼決定`節點`
- Gini impurity
- Information gain
- Variance reduction

...有機會再說吧......

#Association Rules 關聯式規則

## 什麼是關聯式規則？ 
- 用於從大量數據中挖掘出有價值的數據項之間的相關關係
- 不考慮項目的次序，而僅考慮其組合
- 購物籃分析(Market Basket Analysis)
- Apriori演算法是挖掘`布林關聯規則`(Boolean association rules)頻繁項集的算法

## Apriori演算法
<img src="Fig/apriori.png" width="700px">
  
  ## Apriori演算法
  <img src="Fig/apriori.jpg" width="700px">
  
  ## 超市資料分析：讀取資料
  
  ```{r warning=F,message=F,fig.height=4.5}
# Load the libraries
if (!require('arules')){
  install.packages("arules");library(arules)
}
if (!require('datasets')){
  install.packages("datasets");library(datasets)
}
data(Groceries) # Load the data set
Groceries@data@Dim #169 種商品，9835筆交易資料
```

## 超市資料長這樣
<img src="Fig/groceries.png" width="700px">
  
  ## 超市資料分析：先看看什麼賣最好
  
  ```{r warning=F,message=F,fig.height=5}
itemFrequencyPlot(Groceries,topN=20,type="absolute")
```


## 超市資料分析：關聯式分析`apriori()`

```{r warning=F,message=F,fig.height=4.5}
# Get the rules
rules <- apriori(Groceries, 
                 parameter = list(supp = 0.001, conf = 0.8),
                 control = list(verbose=F))
# Show the top 5 rules, but only 2 digits
options(digits=2)
inspect(rules[1:5])
```


## 如何解讀模型
- `Support`: The fraction of which our item set occurs in our dataset. 一次交易中，包括規則內的物品的聯合機率
- `Confidence`: probability that a rule is correct for a new transaction with items on the left. 包含左邊物品A的交易也會包含右邊物品B的條件機率
- `Lift`: The ratio by which by the confidence of a rule exceeds the expected confidence. 規則的信心比期望值高多少
- `lift`=1: items on the left and right are independent.

<img src="Fig/support.png" width="700px">
  
  <img src="Fig/conf.png" width="250px">
  
  <img src="Fig/lift.png" width="400px">
  
  ## 列出最有關連的幾條規則
  ```{r warning=F,message=F,fig.height=4.5}
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
```


## 看看找到的規則們的統計資料
```{r warning=F,message=F,fig.height=4.5}
summary(rules)
```



## 特別針對某項商品，右邊
買了什麼東西的人，會買`牛奶`呢？
```{r warning=F,message=F,fig.height=4.5}
rulesR<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.08), 
                appearance = list(default="lhs",rhs="whole milk"),
                control = list(verbose=F))
rulesR<-sort(rulesR, decreasing=TRUE,by="confidence")
inspect(rulesR[1:5])
```

## 特別針對某項商品，左邊
買了`牛奶`的人，會買什麼呢？
```{r warning=F,message=F,fig.height=4.5}
rulesL<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2), 
                appearance = list(default="rhs",lhs="whole milk"),
                control = list(verbose=F))
rulesL<-sort(rulesL, decreasing=TRUE,by="confidence")
inspect(rulesL[1:5])
```


## 規則視覺化
```{r eval=F,warning=F,message=F,fig.height=4.5}
if (!require('arulesViz')){
  install.packages("arulesViz"); library(arulesViz)
} 
#Mac->http://planspace.org/2013/01/17/fix-r-tcltk-dependency-problem-on-mac/
plot(rules,method="graph",interactive=TRUE,shading=NA) #會跑一陣子
```

<img src="Fig/arulesViz.png" width="400px">  <img src="Fig/arulesVizBig.png" width="300px">
  
  ## 模型驗證？！
  <img src="Fig/SupervisedLearning.png" width="700px">
  
  ## 參考資料
  - 台大資工林軒田教授（田神）：
- [Machine Learning Foundations](www.coursera.org/course/ntumlone)
- [Machine Learning Techniques](www.coursera.org/course/ntumltwo)

- [Market Basket Analysis with R](http://www.salemmarafi.com/code/market-basket-analysis-with-r/)